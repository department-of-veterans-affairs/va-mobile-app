"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[2517],{28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>i});var a=t(96540);const o={},s=a.createContext(o);function r(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),a.createElement(s.Provider,{value:n},e.children)}},73800:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"Engineering/BackEnd/Monitoring/OnCallProcedure","title":"On-Call Procedure","description":"This section outlines the steps to take should an alert occur while it\'s your turn to be on call. There are two sources of alerts: AlertManager and Firebase. Firebase alerts originate from the app. AlertManager alerts originating from Datadog, these alerts are caused by errors or latency from the back-end API.","source":"@site/docs/Engineering/BackEnd/Monitoring/OnCallProcedure.md","sourceDirName":"Engineering/BackEnd/Monitoring","slug":"/Engineering/BackEnd/Monitoring/OnCallProcedure","permalink":"/va-mobile-app/docs/Engineering/BackEnd/Monitoring/OnCallProcedure","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"On-Call Procedure"},"sidebar":"tutorialSidebar","previous":{"title":"Logs","permalink":"/va-mobile-app/docs/Engineering/BackEnd/Monitoring/Logs"},"next":{"title":"Sentry","permalink":"/va-mobile-app/docs/Engineering/BackEnd/Monitoring/Sentry"}}');var o=t(74848),s=t(28453);const r={title:"On-Call Procedure"},i=void 0,l={},c=[{value:"On-Call Rotation",id:"on-call-rotation",level:2},{value:"Handling Backend Alerts",id:"handling-backend-alerts",level:2},{value:"Handling New Issues",id:"handling-new-issues",level:2},{value:"Other Slack channels to monitor",id:"other-slack-channels-to-monitor",level:2}];function d(e){const n={a:"a",code:"code",h2:"h2",li:"li",ol:"ol",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.p,{children:"This section outlines the steps to take should an alert occur while it's your turn to be on call. There are two sources of alerts: AlertManager and Firebase. Firebase alerts originate from the app. AlertManager alerts originating from Datadog, these alerts are caused by errors or latency from the back-end API."}),"\n",(0,o.jsx)(n.h2,{id:"on-call-rotation",children:"On-Call Rotation"}),"\n",(0,o.jsxs)(n.p,{children:["Each week a backend engineer will be on-call. Their on-call hours are the same as their business hours and a slack reminder will show up in ",(0,o.jsx)(n.a,{href:"https://dsva.slack.com/archives/C021WCL114J",children:"va-mobile-app-alerts"})," each Monday tagging whoever is on for that week."]}),"\n",(0,o.jsx)(n.h2,{id:"handling-backend-alerts",children:"Handling Backend Alerts"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["First use tools described above to track down the source of an issue.","\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"https://vagov.ddog-gov.com/apm/services/mobile-app/operations/rack.request/resources?dependencyMap=qson%3A%28data%3A%28telemetrySelection%3Aall_sources%29%2Cversion%3A%210%29&env=eks-prod&fromUser=false&groupMapByOperation=null&panels=qson%3A%28data%3A%28%29%2Cversion%3A%210%29&resources=qson%3A%28data%3A%28visible%3A%21t%2Chits%3A%28selected%3Atotal%29%2Cerrors%3A%28selected%3Atotal%29%2Clatency%3A%28selected%3Ap95%29%2CtopN%3A%215%29%2Cversion%3A%211%29&s3BucketDetails=qson%3A%28data%3A%28%29%2Cversion%3A%210%29&summary=qson%3A%28data%3A%28visible%3A%21t%2Cerrors%3A%28selected%3Acount%29%2Chits%3A%28selected%3Acount%29%2Clatency%3A%28selected%3Alatency%2Cslot%3A%28agg%3A95%29%2Cdistribution%3A%28isLogScale%3A%21f%29%2CshowTraceOutliers%3A%21t%29%2Csublayer%3A%28slot%3A%28layers%3Aservice%29%2Cselected%3Apercentage%29%29%2Cversion%3A%211%29&view=spans&start=1715100779219&end=1715104379219&paused=false",children:"Services in Datadog"})," can show a good overview of the health of our endpoints. This is also a great starting point to dive deeper into various issues."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"https://vagov.ddog-gov.com/logs",children:"Logs in Datadog"})," can help you find more data or trace the requests before the error occurred."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"https://vagov.ddog-gov.com/apm/services/vets-api/operations/rack.request/resources?env=production",children:"Datadog's Application Performance Management tool"})," is also configured for vets-api. It breaks down the ruby, database, and upstream calls down so you can determine the source latency. The APM also provides p50 and p99 latency data to let us know how slow the worst 50% and 1% of calls are doing."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"https://valighthouse.statuspage.io/#",children:"Lighthouse API Status Page"})," is helpful in finding out if lighthouse errors were expected"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["If you've determined that the source of the issue is an upstream service contact the ",(0,o.jsx)(n.a,{href:"/va-mobile-app/docs/Engineering/BackEnd/Architecture/Services#service_contacts",children:"relevant party"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:["If you believe a forward proxy is down or having trouble connecting to the service. Then contact the Operations team via DSVA Slack's ",(0,o.jsx)(n.a,{href:"https://dsva.slack.com/archives/CBU0KDSB1",children:"#vfs-platform-support"})," channel. To open a support ticket type ",(0,o.jsx)(n.code,{children:"/support"}),". This will open a modal window with a form rather than posting a Slack message. For the 'I need help from' field select 'Operations Team'. Then add the details in the 'Summary of request' field. Additionally, if you are unsure of who to contact you can make a support request."]}),"\n",(0,o.jsx)(n.li,{children:"Finally if the error is not from the API, a forward proxy connection to an upstream service, or an upstream service itself but rather an issue with infrastructure that we (and VSP/VFS) control then a SNOW ticket should be opened. Only a DSVA team member can do this. Reach out to a stakeholder and have them open a SNOW ticket for you."}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"handling-new-issues",children:"Handling New Issues"}),"\n",(0,o.jsxs)(n.p,{children:["The on-call engineer is responsible for monitoring the ",(0,o.jsx)(n.a,{href:"https://vagov.ddog-gov.com/monitors/213410",children:"mobile-new-issue-alerts channel"}),", look into each new issue, and determining whether the issue is worth a ticket for further investigation or remediation. The engineer should add a comment to the issue in the channel with either a link to the investigation ticket or an explanation of why it's not worth a ticket. See ",(0,o.jsx)(n.a,{href:"/va-mobile-app/docs/Engineering/BackEnd/Monitoring/DataDog#new-issues-monitor",children:"new issue monitor documentation"})," for more information about new issues."]}),"\n",(0,o.jsx)(n.h2,{id:"other-slack-channels-to-monitor",children:"Other Slack channels to monitor"}),"\n",(0,o.jsx)(n.p,{children:"There are other slack channels that the on-call enginner should pay attention to in case there are updates to maintenance windows and/or other urgent changes. For these channels, the on-call engineer should only need to pay attention to @here and @channel messages."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://dsva.slack.com/archives/CBU0KDSB1",children:"#vfs-platform-support Slack channel"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://dsva.slack.com/archives/C03R5SBELQM",children:"#vfs-change-announcements Slack channel"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://dsva.slack.com/archives/CE4304QPK",children:"#vfs-all-teams Slack channel"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);